{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chunh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from kaggle_environments import evaluate, make, utils\n",
    "from random import choice\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import gym\n",
    "from agents_utils import alphabeta_agent, block_check_agent, create_dql_agent, create_dueling_dql_agent\n",
    "from dql_model import DQNCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / float(len(rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectFourGym(gym.Env):\n",
    "    def __init__(self, agent2=None):\n",
    "        super(ConnectFourGym, self).__init__()\n",
    "        \n",
    "        ks_env = make(\"connectx\", debug=True)\n",
    "        self.env = ks_env.train([None, agent2])\n",
    "        self.rows = ks_env.configuration.rows\n",
    "        self.columns = ks_env.configuration.columns\n",
    "        self.inarow = ks_env.configuration.inarow\n",
    "\n",
    "        # Action and observation space\n",
    "        self.action_space = gym.spaces.Discrete(self.columns)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=2, \n",
    "                                                shape=(1, self.rows, self.columns), dtype=np.int32)\n",
    "\n",
    "        self.reward_range = (-10, 1)\n",
    "        self.spec = None\n",
    "        self.metadata = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.obs = self.env.reset()\n",
    "        board = np.array(self.obs['board']).reshape(1, self.rows, self.columns)\n",
    "        return board.astype(np.float32)  \n",
    "\n",
    "    def step(self, action):\n",
    "        # Check if the move is valid \n",
    "        is_valid = self.obs['board'][action] == 0\n",
    "        if is_valid:\n",
    "            self.obs, old_reward, done, _ = self.env.step(action)\n",
    "            reward = self._custom_reward(old_reward, done)\n",
    "        else:\n",
    "            # Penalize invalid moves\n",
    "            reward, done, _ = -10, True, {}\n",
    "\n",
    "        board = np.array(self.obs['board']).reshape(1, self.rows, self.columns).astype(np.float32)\n",
    "        return board, reward, done, _\n",
    "\n",
    "    def _custom_reward(self, old_reward, done):\n",
    "        if old_reward == 1:      # Win\n",
    "            return 1.0\n",
    "        elif done:               # Loss\n",
    "            return -1.0\n",
    "        else:                    # Neutral move\n",
    "            return 1.0 / (self.rows * self.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*transitions)\n",
    "        return (np.array(states), actions, rewards, np.array(next_states), dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_double_dql(env, episodes=1_000_000, gamma=0.99, epsilon=1.0, epsilon_decay=0.9999, \n",
    "              min_epsilon=0.01, batch_size=128, buffer_size=100_000, target_update=1000, \n",
    "              model_save_path=\"./trained_models/double_dql_b1g_1M_episodes.pt\"):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_shape = (1, env.rows, env.columns)\n",
    "    n_actions = env.columns\n",
    "\n",
    "    policy_net = DQNCNN(input_shape, n_actions).to(device)\n",
    "    target_net = DQNCNN(input_shape, n_actions).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=0.0005)\n",
    "    replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "            # Îµ-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randint(0, n_actions - 1)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = policy_net(state_tensor)\n",
    "                    action = q_values.argmax().item()\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            # Sample and train\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                states_tensor = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "                actions_tensor = torch.tensor(actions).unsqueeze(1).to(device)\n",
    "                rewards_tensor = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "                next_states_tensor = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
    "                dones_tensor = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "                # Q(s, a)\n",
    "                q_values = policy_net(states_tensor).gather(1, actions_tensor)\n",
    "\n",
    "                # Double DQN target: Q(s', argmax_a' Q(s', a'; policy), a'; target)\n",
    "                with torch.no_grad():\n",
    "                    next_actions = policy_net(next_states_tensor).argmax(1, keepdim=True)\n",
    "                    next_q_values = target_net(next_states_tensor).gather(1, next_actions)\n",
    "\n",
    "                target = rewards_tensor + gamma * next_q_values * (1 - dones_tensor)\n",
    "\n",
    "                loss = F.mse_loss(q_values, target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "        if episode % target_update == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        print(f\"Episode {episode+1}, Total reward: {total_reward}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    torch.save(policy_net.state_dict(), model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "double_dql_agent_b1 = create_dql_agent(\"./trained_models/double_dql_blockcheck_1.pt\")\n",
    "double_dql_agent_b1e = create_dql_agent(\"./trained_models/double_dql_blockcheck_1e.pt\")\n",
    "dueling_dql_agent_1 = create_dueling_dql_agent(\"./trained_models/dueling_dql_blockcheck_1.pt\")\n",
    "double_dql_agent_b1f = create_dql_agent(\"./trained_models/double_dql_blockcheck_1f.pt\")\n",
    "double_dql_agent_b1g = create_dql_agent(\"./trained_models/double_dql_b1g_1M_episodes.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total reward: -0.8809523809523809, Epsilon: 1.000\n",
      "Episode 2, Total reward: -0.8333333333333334, Epsilon: 1.000\n",
      "Episode 3, Total reward: -0.7380952380952381, Epsilon: 1.000\n",
      "Episode 4, Total reward: -9.738095238095237, Epsilon: 1.000\n",
      "Episode 5, Total reward: -9.857142857142858, Epsilon: 1.000\n",
      "Episode 6, Total reward: -0.8333333333333334, Epsilon: 0.999\n",
      "Episode 7, Total reward: -0.7857142857142857, Epsilon: 0.999\n",
      "Episode 8, Total reward: -9.833333333333334, Epsilon: 0.999\n",
      "Episode 9, Total reward: -0.7142857142857143, Epsilon: 0.999\n",
      "Episode 10, Total reward: -0.8333333333333334, Epsilon: 0.999\n",
      "Episode 11, Total reward: -0.8333333333333334, Epsilon: 0.999\n",
      "Episode 12, Total reward: -0.8333333333333334, Epsilon: 0.999\n",
      "Episode 13, Total reward: -0.6666666666666666, Epsilon: 0.999\n",
      "Episode 14, Total reward: -0.7619047619047619, Epsilon: 0.999\n",
      "Episode 15, Total reward: -0.8333333333333334, Epsilon: 0.999\n",
      "Episode 16, Total reward: -0.7619047619047619, Epsilon: 0.998\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m ConnectFourGym(agent2\u001b[38;5;241m=\u001b[39mblock_check_agent)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_double_dql\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 42\u001b[0m, in \u001b[0;36mtrain_double_dql\u001b[1;34m(env, episodes, gamma, epsilon, epsilon_decay, min_epsilon, batch_size, buffer_size, target_update, model_save_path)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Sample and train\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(replay_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[1;32m---> 42\u001b[0m     states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m \u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     states_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(states, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     45\u001b[0m     actions_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(actions)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m      9\u001b[0m transitions \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer, batch_size)\n\u001b[0;32m     10\u001b[0m states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mtransitions)\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m, actions, rewards, np\u001b[38;5;241m.\u001b[39marray(next_states), dones)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = ConnectFourGym(agent2=block_check_agent)\n",
    "train_double_dql(env, episodes=1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_win_percentages(agent1, agent2, n_rounds=100):\n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "    \n",
    "    # Agent 1 goes first half the time\n",
    "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
    "    \n",
    "    # Agent 2 goes first the other half \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "\n",
    "    total = len(outcomes)\n",
    "    agent1_wins = outcomes.count([1, -1])\n",
    "    agent2_wins = outcomes.count([-1, 1])\n",
    "    ties = outcomes.count([0, 0])\n",
    "    invalid_1 = outcomes.count([None, 0])\n",
    "    invalid_2 = outcomes.count([0, None])\n",
    "\n",
    "    print(f\"Total games: {total}\")\n",
    "    print(f\"Agent 1 Win Percentage: {agent1_wins / total:.2%}\")\n",
    "    print(f\"Agent 2 Win Percentage: {agent2_wins / total:.2%}\")\n",
    "    print(f\"Tie Percentage: {ties / total:.2%}\")\n",
    "    print(f\"Invalid plays by Agent 1: {invalid_1}\")\n",
    "    print(f\"Invalid plays by Agent 2: {invalid_2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total games: 200\n",
      "Agent 1 Win Percentage: 0.00%\n",
      "Agent 2 Win Percentage: 100.00%\n",
      "Tie Percentage: 0.00%\n",
      "Invalid plays by Agent 1: 0\n",
      "Invalid plays by Agent 2: 0\n"
     ]
    }
   ],
   "source": [
    "get_win_percentages(double_dql_agent_b1g, alphabeta_agent, n_rounds=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block check agent vs Random Agent: 0.78\n",
      "Block check agent vs Negamax Agent: -1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Block check agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [double_dql_agent_b1, \"random\"], num_episodes=100)))\n",
    "print(\"Block check agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [double_dql_agent_b1, dql_agent_n1], num_episodes=100)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
