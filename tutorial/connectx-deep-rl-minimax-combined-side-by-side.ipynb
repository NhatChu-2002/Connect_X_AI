{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17592,"databundleVersionId":899221,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nForked from the tutorial notebook. In the tutorial, you learned a bit about reinforcement learning and used the `stable-baselines3` package to train an agent to beat a random opponent. Now we will create a combined agent which uses both Deep Reinforcement Learning & MiniMax with alpha-beta pruning\n\n**References & Credits:**\n\n* Pretrained models: https://www.kaggle.com/code/salmtcat/ppo-vs-a2c-vs-dqn-and-mlppolicy-vs-cnnpolicy/notebook\n* Minimax: https://www.kaggle.com/code/syedjarullahhisham/connectx-n-step-lookahead-minimax","metadata":{}},{"cell_type":"code","source":"from learntools.core import binder\nbinder.bind(globals())\nfrom learntools.game_ai.ex4 import *","metadata":{"execution":{"iopub.status.busy":"2024-01-04T08:02:46.386345Z","iopub.execute_input":"2024-01-04T08:02:46.386776Z","iopub.status.idle":"2024-01-04T08:02:46.392251Z","shell.execute_reply.started":"2024-01-04T08:02:46.386746Z","shell.execute_reply":"2024-01-04T08:02:46.390850Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import gym\nfrom kaggle_environments import make, evaluate\nfrom gym import spaces\n\nimport random\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch as th\nfrom torch import nn as nn\nimport torch.nn.functional as F\nimport torch\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# !pip install \"stable-baselines3\"\nfrom stable_baselines3 import PPO\nfrom stable_baselines3 import A2C\nfrom stable_baselines3 import DQN\n\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.monitor import load_results\nfrom stable_baselines3.common.torch_layers import NatureCNN\nfrom stable_baselines3.common.policies import ActorCriticPolicy, ActorCriticCnnPolicy\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.env_checker import check_env\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-01-04T08:02:48.484384Z","iopub.execute_input":"2024-01-04T08:02:48.484759Z","iopub.status.idle":"2024-01-04T08:03:08.253802Z","shell.execute_reply.started":"2024-01-04T08:02:48.484730Z","shell.execute_reply":"2024-01-04T08:03:08.251100Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"No pygame installed, ignoring import\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Setup Environment","metadata":{}},{"cell_type":"code","source":"class ConnectFourGym(gym.Env):\n    def __init__(self, agent2=\"random\"):\n        ks_env = make(\"connectx\", debug=True)\n        self.env = ks_env.train([None, agent2])\n        self.rows = ks_env.configuration.rows\n        self.columns = ks_env.configuration.columns\n        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n        self.action_space = gym.spaces.Discrete(self.columns)\n        self.observation_space = gym.spaces.Box(low=0, high=2, \n                                            shape=(1,self.rows,self.columns), dtype=int)\n        # Tuple corresponding to the min and max possible rewards\n        self.reward_range = (-10, 1)\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n    def reset(self):\n        self.obs = self.env.reset()\n        return np.array(self.obs['board']).reshape(1,self.rows,self.columns)\n    def change_reward(self, old_reward, done):\n        if old_reward == 1: # The agent won the game\n            return 1\n        elif done: # The opponent won the game\n            return -1\n        else: # Reward 1/42\n            return 1/(self.rows*self.columns)\n    def step(self, action):\n        # Check if agent's move is valid\n        is_valid = (self.obs['board'][int(action)] == 0)\n        if is_valid: # Play the move\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done)\n        else: # End the game and penalize agent\n            reward, done, _ = -10, True, {}\n        return np.array(self.obs['board']).reshape(1,self.rows,self.columns), reward, done, _","metadata":{"execution":{"iopub.status.busy":"2024-01-04T08:03:18.810533Z","iopub.execute_input":"2024-01-04T08:03:18.811230Z","iopub.status.idle":"2024-01-04T08:03:18.822300Z","shell.execute_reply.started":"2024-01-04T08:03:18.811198Z","shell.execute_reply":"2024-01-04T08:03:18.821281Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Create ConnectFour environment \nenv = ConnectFourGym(agent2='negamax')\n\n# Create directory for logging training information\nlog_dir = \"log/\"\nos.makedirs(log_dir, exist_ok=True)\n\n# Vectorize the environment so that algorithms work correctly \n# env = Monitor(env, log_dir, allow_early_resets=True)\nenv = DummyVecEnv([lambda: env])","metadata":{"execution":{"iopub.status.busy":"2024-01-04T08:03:22.589685Z","iopub.execute_input":"2024-01-04T08:03:22.590037Z","iopub.status.idle":"2024-01-04T08:03:22.672199Z","shell.execute_reply.started":"2024-01-04T08:03:22.590014Z","shell.execute_reply":"2024-01-04T08:03:22.671199Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Win Percentage Calculation","metadata":{}},{"cell_type":"code","source":"from kaggle_environments import make, evaluate\nimport numpy as np\n\ndef evaluate_agent(agent1, agent2, n_rounds=100):\n    # Use default Connect Four setup\n    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n    # Agent 1 goes first (roughly) half the time          \n    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n    # Agent 2 goes first (roughly) half the time      \n    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n    \n    agent1_win = np.round(outcomes.count([1,-1])/len(outcomes), 2)\n    agent2_win = np.round(outcomes.count([-1,1])/len(outcomes), 2)\n    print(\"Agent 1 Win Percentage:\", agent1_win)\n    print(\"Agent 2 Win Percentage:\", agent2_win)\n    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))","metadata":{"execution":{"iopub.status.busy":"2024-01-04T08:03:26.885905Z","iopub.execute_input":"2024-01-04T08:03:26.886264Z","iopub.status.idle":"2024-01-04T08:03:26.896303Z","shell.execute_reply.started":"2024-01-04T08:03:26.886236Z","shell.execute_reply":"2024-01-04T08:03:26.894557Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Setup Architecture","metadata":{}},{"cell_type":"code","source":"# Neural network for predicting action values\nclass CustomCNN(BaseFeaturesExtractor):\n    \n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int=128):\n        super(CustomCNN, self).__init__(observation_space, features_dim)\n        # CxHxW images (channels first)\n        n_input_channels = observation_space.shape[0]\n        self.cnn = nn.Sequential(\n            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n            nn.Flatten(),\n        )\n\n        # Compute shape by doing one forward pass\n        with th.no_grad():\n            n_flatten = self.cnn(\n                th.as_tensor(observation_space.sample()[None]).float()\n            ).shape[1]\n\n        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        return self.linear(self.cnn(observations))\n\npolicy_kwargs = dict(\n    features_extractor_class=CustomCNN,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T08:03:30.092647Z","iopub.execute_input":"2024-01-04T08:03:30.093274Z","iopub.status.idle":"2024-01-04T08:03:30.102453Z","shell.execute_reply.started":"2024-01-04T08:03:30.093247Z","shell.execute_reply":"2024-01-04T08:03:30.100676Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"def train_model(model, name):\n    \n    # First run -> untrained model, afterwards -> trained\n    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n    print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n    best_mean_reward = mean_reward\n    \n    eval_data_df = pd.DataFrame()\n\n    for i in range(1,5):\n        print('Learning step:',i)\n        model.learn(total_timesteps=12000,reset_num_timesteps=False)\n        print('evaluate_policy:',i)\n        mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n        print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n\n        if mean_reward > best_mean_reward:\n            print('Improved!')\n            best_mean_reward = mean_reward\n            model.save(f\"{name}\")\n        else:\n            print(f\"Current best:{best_mean_reward:.2f}\")\n        \n        row_df = pd.DataFrame({'mean_reward': [mean_reward],\n                               'std_reward': [std_reward],\n                               'iteration':[i]},index=[i])\n\n        if eval_data_df.empty:\n            eval_data_df = row_df\n        else:\n            eval_data_df=pd.concat([eval_data_df,row_df])\n            eval_data_df['mean_roll_average'] = eval_data_df.mean_reward.rolling(15, min_periods=1).mean()\n\n            eval_data_df['mean_roll_average']=eval_data_df['mean_roll_average'].fillna(eval_data_df['mean_reward'])\n\n            plt.figure(figsize=(12,3))\n            sns.lineplot(x=eval_data_df['iteration'],y=eval_data_df['mean_reward'], label=\"reward\")\n            sns.lineplot(x=eval_data_df['iteration'],y=eval_data_df['mean_roll_average'], label=\"average\");\n            plt.show()\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-01-04T08:03:40.184227Z","iopub.execute_input":"2024-01-04T08:03:40.185693Z","iopub.status.idle":"2024-01-04T08:03:40.194961Z","shell.execute_reply.started":"2024-01-04T08:03:40.185644Z","shell.execute_reply":"2024-01-04T08:03:40.193815Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Different Agents Builder","metadata":{}},{"cell_type":"markdown","source":"# DQN","metadata":{}},{"cell_type":"markdown","source":"## MLP Policy ","metadata":{}},{"cell_type":"code","source":"def train_DQN_MLP():\n    DQN_MLP = DQN('MlpPolicy', env, policy_kwargs=policy_kwargs)\n\n    if os.path.exists('DQN_MLP.zip'):\n        print('file found')\n        DQN_MLP = DQN.load('DQN_MLP', env=env)\n        \n    print('DQN MLP')\n    DQN_MLP = train_model(DQN_MLP,'DQN MLP')\n    print(DQN_MLP.policy)\n\ndef DQN_MLP_Agent(obs, config):\n    obs = np.array(obs['board']).reshape(1, config.rows, config.columns)\n    action, _ = DQN_MLP.predict(obs)\n    return int(action)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T08:03:43.289303Z","iopub.execute_input":"2024-01-04T08:03:43.289692Z","iopub.status.idle":"2024-01-04T08:03:43.296699Z","shell.execute_reply.started":"2024-01-04T08:03:43.289663Z","shell.execute_reply":"2024-01-04T08:03:43.295340Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## CNN Policy ","metadata":{}},{"cell_type":"code","source":"def train_DQN_CNN():\n    DQN_CNN = DQN('CnnPolicy', env, policy_kwargs=policy_kwargs)\n\n    if os.path.exists('DQN_CNN.zip'):\n        print('file found')\n        DQN_CNN = DQN.load('DQN_CNN', env=env)\n    \n    print('DQN CNN')\n    DQN_CNN = train_model(DQN_CNN,'DQN CNN')    \n    print(DQN_CNN.policy)\n\ndef DQN_CNN_Agent(obs, config):\n    obs = np.array(obs['board']).reshape(1, config.rows, config.columns)\n    action, _ = DQN_CNN.predict(obs)\n    return int(action)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T08:03:45.741200Z","iopub.execute_input":"2024-01-04T08:03:45.741655Z","iopub.status.idle":"2024-01-04T08:03:45.749216Z","shell.execute_reply.started":"2024-01-04T08:03:45.741620Z","shell.execute_reply":"2024-01-04T08:03:45.747239Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# PPO","metadata":{}},{"cell_type":"markdown","source":"## MLP Policy ","metadata":{}},{"cell_type":"code","source":"def train_PPO_MLP():\n    PPO_MLP = PPO('MlpPolicy', env, policy_kwargs=policy_kwargs)\n\n    if os.path.exists('PPO_MLP.zip'):\n        print('file found')\n        PPO_MLP = PPO.load('PPO_MLP', env=env)\n    \n    print('PPO MLP')\n    PPO_MLP = train_model(PPO_MLP,'PPO MLP')    \n    print(PPO_MLP.policy)\n\ndef PPO_MLP_Agent(obs, config):\n    obs = np.array(obs['board']).reshape(1, config.rows, config.columns)\n    action, _ = PPO_MLP.predict(obs)\n    return int(action)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T08:03:48.232880Z","iopub.execute_input":"2024-01-04T08:03:48.233252Z","iopub.status.idle":"2024-01-04T08:03:48.239274Z","shell.execute_reply.started":"2024-01-04T08:03:48.233222Z","shell.execute_reply":"2024-01-04T08:03:48.238045Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## CNN Policy ","metadata":{}},{"cell_type":"code","source":"def train_PPO_CNN():\n    PPO_CNN = PPO('CnnPolicy', env, policy_kwargs=policy_kwargs)\n\n    if os.path.exists('PPO_CNN.zip'):\n        print('file found')\n        PPO_CNN = PPO.load('PPO_CNN', env=env)\n\n    print('PPO CNN')\n    PPO_CNN = train_model(PPO_CNN,'PPO CNN')\n    print(PPO_CNN.policy)\n\ndef PPO_CNN_Agent(obs, config):\n    obs = np.array(obs['board']).reshape(1, config.rows, config.columns)\n    action, _ = PPO_CNN.predict(obs)\n    return int(action)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T08:03:50.936610Z","iopub.execute_input":"2024-01-04T08:03:50.936966Z","iopub.status.idle":"2024-01-04T08:03:50.943808Z","shell.execute_reply.started":"2024-01-04T08:03:50.936939Z","shell.execute_reply":"2024-01-04T08:03:50.942581Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Train Different Agents","metadata":{}},{"cell_type":"markdown","source":"This should only be run first time. After that, we will just load previously trained agents","metadata":{}},{"cell_type":"code","source":"train_DQN_MLP()\ntrain_DQN_CNN()\ntrain_PPO_MLP()\ntrain_PPO_CNN()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Saved Agents","metadata":{}},{"cell_type":"markdown","source":"Stable baseline only supports zip files to load. If this notebook is run for training all agents, then that's not a problem as all the trained agents saved as zip in working folder. But, training agent each time requires hours to run. So, I added the pretrained agents in the input folder from previous version of notebook. Here is the catch. By default, Kaggle unzip the zipped folders automatically while adding zip files. But stable baseline can only process zip files. That's why, I first make zip files of the models at the working folder using shutil library, then load from there. Here is the code","metadata":{}},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"DQN_MLP\", 'zip', \"/kaggle/input/deep-rl-connectx/DQN MLP\")\nshutil.make_archive(\"DQN_CNN\", 'zip', \"/kaggle/input/deep-rl-connectx/DQN CNN\")\nshutil.make_archive(\"PPO_MLP\", 'zip', \"/kaggle/input/deep-rl-connectx/PPO MLP\")\nshutil.make_archive(\"PPO_CNN\", 'zip', \"/kaggle/input/deep-rl-connectx/PPO CNN\")","metadata":{"execution":{"iopub.status.busy":"2024-01-04T08:03:54.838765Z","iopub.execute_input":"2024-01-04T08:03:54.839112Z","iopub.status.idle":"2024-01-04T08:03:55.256835Z","shell.execute_reply.started":"2024-01-04T08:03:54.839082Z","shell.execute_reply":"2024-01-04T08:03:55.255117Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/PPO_CNN.zip'"},"metadata":{}}]},{"cell_type":"code","source":"DQN_MLP = DQN('MlpPolicy', env, policy_kwargs=policy_kwargs)\nDQN_MLP.set_parameters('DQN_MLP')\nDQN_CNN = DQN('CnnPolicy', env, policy_kwargs=policy_kwargs)\nDQN_CNN.set_parameters('DQN_CNN')\n\nPPO_MLP = PPO('MlpPolicy', env, policy_kwargs=policy_kwargs)\nPPO_MLP.set_parameters('PPO_MLP')\nPPO_CNN = PPO('CnnPolicy', env, policy_kwargs=policy_kwargs)\nPPO_CNN.set_parameters('PPO_CNN')","metadata":{"execution":{"iopub.status.busy":"2024-01-04T08:04:06.853638Z","iopub.execute_input":"2024-01-04T08:04:06.854005Z","iopub.status.idle":"2024-01-04T08:04:07.100909Z","shell.execute_reply.started":"2024-01-04T08:04:06.853977Z","shell.execute_reply":"2024-01-04T08:04:07.099820Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# War Between Agents","metadata":{}},{"cell_type":"markdown","source":"## Between RL Saved Agents ","metadata":{}},{"cell_type":"code","source":"num_episodes = 1000\n\nprint('PPO_MLP_Agent vs PPO_CNN_Agent')\nevaluate_agent(PPO_MLP_Agent, PPO_CNN_Agent, n_rounds=num_episodes)\nprint('DQN_MLP_Agent vs DQN_CNN_Agent')\nevaluate_agent(DQN_MLP_Agent, DQN_CNN_Agent, n_rounds=num_episodes)\n\nprint('PPO_MLP_Agent vs DQN_MLP_Agent')\nevaluate_agent(PPO_MLP_Agent, DQN_MLP_Agent, n_rounds=num_episodes)\nprint('PPO_CNN_Agent vs DQN_CNN_Agent')\nevaluate_agent(PPO_CNN_Agent, DQN_CNN_Agent, n_rounds=num_episodes)\n\nprint('PPO_MLP_Agent vs DQN_CNN_Agent')\nevaluate_agent(PPO_MLP_Agent, DQN_CNN_Agent, n_rounds=num_episodes)\nprint('PPO_CNN_Agent vs DQN_MLP_Agent')\nevaluate_agent(PPO_CNN_Agent, DQN_MLP_Agent, n_rounds=num_episodes)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T09:17:54.353039Z","iopub.execute_input":"2024-01-04T09:17:54.353403Z","iopub.status.idle":"2024-01-04T09:23:09.353347Z","shell.execute_reply.started":"2024-01-04T09:17:54.353375Z","shell.execute_reply":"2024-01-04T09:23:09.352140Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"PPO_MLP_Agent vs PPO_CNN_Agent\nAgent 1 Win Percentage: 0.43\nAgent 2 Win Percentage: 0.42\nNumber of Invalid Plays by Agent 1: 73\nNumber of Invalid Plays by Agent 2: 71\nDQN_MLP_Agent vs DQN_CNN_Agent\nAgent 1 Win Percentage: 0.5\nAgent 2 Win Percentage: 0.5\nNumber of Invalid Plays by Agent 1: 0\nNumber of Invalid Plays by Agent 2: 0\nPPO_MLP_Agent vs DQN_MLP_Agent\nAgent 1 Win Percentage: 0.02\nAgent 2 Win Percentage: 0.88\nNumber of Invalid Plays by Agent 1: 3\nNumber of Invalid Plays by Agent 2: 100\nPPO_CNN_Agent vs DQN_CNN_Agent\nAgent 1 Win Percentage: 0.03\nAgent 2 Win Percentage: 0.95\nNumber of Invalid Plays by Agent 1: 1\nNumber of Invalid Plays by Agent 2: 19\nPPO_MLP_Agent vs DQN_CNN_Agent\nAgent 1 Win Percentage: 0.02\nAgent 2 Win Percentage: 0.97\nNumber of Invalid Plays by Agent 1: 0\nNumber of Invalid Plays by Agent 2: 3\nPPO_CNN_Agent vs DQN_MLP_Agent\nAgent 1 Win Percentage: 0.04\nAgent 2 Win Percentage: 0.8\nNumber of Invalid Plays by Agent 1: 7\nNumber of Invalid Plays by Agent 2: 155\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## RL Agents Against random agents ","metadata":{}},{"cell_type":"code","source":"print('PPO_MLP_Agent vs Random Agent')\nevaluate_agent(PPO_MLP_Agent, \"random\", n_rounds=num_episodes)\nprint('PPO_CNN_Agent vs Random Agent')\nevaluate_agent(PPO_CNN_Agent, \"random\", n_rounds=num_episodes)\nprint('DQN_MLP_Agent vs Random Agent')\nevaluate_agent(DQN_MLP_Agent, \"random\", n_rounds=num_episodes)\nprint('DQN_CNN_Agent vs Random Agent')\nevaluate_agent(DQN_CNN_Agent, \"random\", n_rounds=num_episodes)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T08:51:57.690942Z","iopub.execute_input":"2024-01-04T08:51:57.691334Z","iopub.status.idle":"2024-01-04T08:52:21.832182Z","shell.execute_reply.started":"2024-01-04T08:51:57.691301Z","shell.execute_reply":"2024-01-04T08:52:21.831445Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"PPO_MLP_Agent vs Random Agent\nAgent 1 Win Percentage: 0.56\nAgent 2 Win Percentage: 0.35\nNumber of Invalid Plays by Agent 1: 9\nNumber of Invalid Plays by Agent 2: 0\nPPO_CNN_Agent vs Random Agent\nAgent 1 Win Percentage: 0.58\nAgent 2 Win Percentage: 0.3\nNumber of Invalid Plays by Agent 1: 12\nNumber of Invalid Plays by Agent 2: 0\nDQN_MLP_Agent vs Random Agent\nAgent 1 Win Percentage: 0.72\nAgent 2 Win Percentage: 0.0\nNumber of Invalid Plays by Agent 1: 28\nNumber of Invalid Plays by Agent 2: 0\nDQN_CNN_Agent vs Random Agent\nAgent 1 Win Percentage: 0.63\nAgent 2 Win Percentage: 0.05\nNumber of Invalid Plays by Agent 1: 32\nNumber of Invalid Plays by Agent 2: 0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## RL Agents Against negamax agents ","metadata":{}},{"cell_type":"code","source":"num_episodes = 100  # as these tests take time to run\nprint('PPO_MLP_Agent vs Negamax Agent')\nevaluate_agent(PPO_MLP_Agent, \"negamax\", n_rounds=num_episodes)\nprint('PPO_CNN_Agent vs Negamax Agent')\nevaluate_agent(PPO_CNN_Agent, \"negamax\", n_rounds=num_episodes)\nprint('DQN_MLP_Agent vs Negamax Agent')\nevaluate_agent(DQN_MLP_Agent, \"negamax\", n_rounds=num_episodes)\nprint('DQN_CNN_Agent vs Negamax Agent')\nevaluate_agent(DQN_CNN_Agent, \"negamax\", n_rounds=num_episodes)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T09:00:37.264430Z","iopub.execute_input":"2024-01-04T09:00:37.264832Z","iopub.status.idle":"2024-01-04T09:06:15.804782Z","shell.execute_reply.started":"2024-01-04T09:00:37.264800Z","shell.execute_reply":"2024-01-04T09:06:15.803852Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"PPO_MLP_Agent vs Negamax Agent\nAgent 1 Win Percentage: 0.01\nAgent 2 Win Percentage: 0.99\nNumber of Invalid Plays by Agent 1: 0\nNumber of Invalid Plays by Agent 2: 0\nPPO_CNN_Agent vs Negamax Agent\nAgent 1 Win Percentage: 0.01\nAgent 2 Win Percentage: 0.97\nNumber of Invalid Plays by Agent 1: 2\nNumber of Invalid Plays by Agent 2: 0\nDQN_MLP_Agent vs Negamax Agent\nAgent 1 Win Percentage: 0.0\nAgent 2 Win Percentage: 0.51\nNumber of Invalid Plays by Agent 1: 49\nNumber of Invalid Plays by Agent 2: 0\nDQN_CNN_Agent vs Negamax Agent\nAgent 1 Win Percentage: 0.0\nAgent 2 Win Percentage: 0.06\nNumber of Invalid Plays by Agent 1: 94\nNumber of Invalid Plays by Agent 2: 0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Against My Last Best Agent(Minimax Agent)","metadata":{}},{"cell_type":"markdown","source":"I created agents in previous tutorials from which the minimax with alpha-beta pruning runs best. We will use it as custom agent to evaluate our new agent","metadata":{}},{"cell_type":"code","source":"def my_minimax_agent(obs, config):\n    import random\n    import numpy as np\n    import multiprocessing\n    from functools import partial\n    \n    # How deep to make the game tree: higher values take longer to run!\n    N_STEPS = 3\n    \n    # Helper function for minimax: calculates value of heuristic for grid\n    def get_heuristic(grid, mark, config):\n        A = 1e6\n        B = 1e2\n        C = 1\n        D = -1e3\n        E = -1e11\n        num_twos = count_windows(grid, 2, mark, config)\n        num_threes = count_windows(grid, 3, mark, config)\n        num_fours = count_windows(grid, 4, mark, config)\n        num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n        num_fours_opp = count_windows(grid, 4, mark%2+1, config)\n        score = D*num_threes_opp + E*num_fours_opp + A*num_fours + B*num_threes + C*num_twos\n        return score\n    \n    # Helper function for get_heuristic: checks if window satisfies heuristic conditions\n    def check_window(window, num_discs, piece, config):\n        return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n\n    # Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\n    def count_windows(grid, num_discs, piece, config):\n        num_windows = 0\n        # horizontal\n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[row, col:col+config.inarow])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(grid[row:row+config.inarow, col])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        return num_windows\n    \n    # Helper function for score_move: gets board at next step if agent drops piece in selected column\n    def drop_piece(grid, col, mark, config):\n        next_grid = grid.copy()\n        for row in range(config.rows-1, -1, -1):\n            if next_grid[row][col] == 0:\n                break\n        next_grid[row][col] = mark\n        return next_grid\n    \n    # Uses minimax to calculate value of dropping piece in selected column\n    def score_move(grid, col, mark, config, nsteps):\n        next_grid = drop_piece(grid, col, mark, config)\n        score = minimax(next_grid, nsteps-1, False, mark, config,-np.inf, np.inf)\n        return score\n\n    # Helper function for minimax: checks if agent or opponent has four in a row in the window\n    def is_terminal_window(window, config):\n        return window.count(1) == config.inarow or window.count(2) == config.inarow\n\n    # Helper function for minimax: checks if game has ended\n    def is_terminal_node(grid, config):\n        # Check for draw \n        if list(grid[0, :]).count(0) == 0:\n            return True\n        # Check for win: horizontal, vertical, or diagonal\n        # horizontal \n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[row, col:col+config.inarow])\n                if is_terminal_window(window, config):\n                    return True\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(grid[row:row+config.inarow, col])\n                if is_terminal_window(window, config):\n                    return True\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if is_terminal_window(window, config):\n                    return True\n        # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if is_terminal_window(window, config):\n                    return True\n        return False\n\n    # Minimax implementation\n    def minimax(node, depth, maximizingPlayer, mark, config, alpha, beta):\n        is_terminal = is_terminal_node(node, config)\n        valid_moves = [c for c in range(config.columns) if node[0][c] == 0]\n        if depth == 0 or is_terminal:\n            return get_heuristic(node, mark, config)\n        if maximizingPlayer:\n            value = -np.Inf\n            for col in valid_moves:\n                child = drop_piece(node, col, mark, config)\n                value = max(value, minimax(child, depth-1, False, mark, config, alpha, beta))\n                alpha = max(alpha, value)\n                if beta <= alpha:\n                    break\n            return value\n        else:\n            value = np.Inf\n            for col in valid_moves:\n                child = drop_piece(node, col, mark%2+1, config)\n                value = min(value, minimax(child, depth-1, True, mark, config, alpha, beta))\n                beta = min(beta, value)\n                if beta <= alpha:\n                    break\n            return value\n        \n    # Get list of valid moves\n    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n    # Convert the board to a 2D grid\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    # Use the heuristic to assign a score to each possible board in the next step\n    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config, N_STEPS) for col in valid_moves]))\n    # Get a list of columns (moves) that maximize the heuristic\n    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n    # Select at random from the maximizing columns\n    return random.choice(max_cols)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T09:06:15.806913Z","iopub.execute_input":"2024-01-04T09:06:15.807756Z","iopub.status.idle":"2024-01-04T09:06:15.833025Z","shell.execute_reply.started":"2024-01-04T09:06:15.807719Z","shell.execute_reply":"2024-01-04T09:06:15.831943Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"print('PPO_MLP_Agent vs Minimax Agent')\nevaluate_agent(PPO_MLP_Agent, my_minimax_agent, n_rounds=num_episodes)\nprint('PPO_CNN_Agent vs Minimax Agent')\nevaluate_agent(PPO_CNN_Agent, my_minimax_agent, n_rounds=num_episodes)\nprint('DQN_MLP_Agent vs Minimax Agent')\nevaluate_agent(DQN_MLP_Agent, my_minimax_agent, n_rounds=num_episodes)\nprint('DQN_CNN_Agent vs Minimax Agent')\nevaluate_agent(DQN_CNN_Agent, my_minimax_agent, n_rounds=num_episodes)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T09:06:15.834291Z","iopub.execute_input":"2024-01-04T09:06:15.835126Z","iopub.status.idle":"2024-01-04T09:17:18.403602Z","shell.execute_reply.started":"2024-01-04T09:06:15.835094Z","shell.execute_reply":"2024-01-04T09:17:18.402135Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"PPO_MLP_Agent vs Minimax Agent\nAgent 1 Win Percentage: 0.01\nAgent 2 Win Percentage: 0.99\nNumber of Invalid Plays by Agent 1: 0\nNumber of Invalid Plays by Agent 2: 0\nPPO_CNN_Agent vs Minimax Agent\nAgent 1 Win Percentage: 0.0\nAgent 2 Win Percentage: 1.0\nNumber of Invalid Plays by Agent 1: 0\nNumber of Invalid Plays by Agent 2: 0\nDQN_MLP_Agent vs Minimax Agent\nAgent 1 Win Percentage: 0.0\nAgent 2 Win Percentage: 0.2\nNumber of Invalid Plays by Agent 1: 80\nNumber of Invalid Plays by Agent 2: 0\nDQN_CNN_Agent vs Minimax Agent\nAgent 1 Win Percentage: 0.0\nAgent 2 Win Percentage: 0.62\nNumber of Invalid Plays by Agent 1: 38\nNumber of Invalid Plays by Agent 2: 0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Best RL Agent With MiniMax Submission Preparation","metadata":{}},{"cell_type":"markdown","source":"https://www.kaggle.com/code/salmtcat/ppo-vs-a2c-vs-dqn-and-mlppolicy-vs-cnnpolicy/notebook#Submission-file","metadata":{}},{"cell_type":"markdown","source":"Here we will combine our best RL agent DQN_MLP with our previous best minimax agent as submission agent. As we see our RL agent take a lot of invalid moves. We will replace those invalid moves with minimax moves.","metadata":{}},{"cell_type":"code","source":"%%writefile submission.py\n\ndef agent(obs, config):\n    import numpy as np\n    import torch as th\n    from torch import nn as nn\n    import torch.nn.functional as F\n    from torch import tensor\n    import random\n    \n    class Net(nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.cnn0 = nn.Conv2d(1, 32, kernel_size=3)\n            self.cnn2 = nn.Conv2d(32, 64, kernel_size=3)\n            self.linear0 = nn.Linear(384, 128)\n            self.qnet0 = nn.Linear(128, 64)\n            self.qnet2 = nn.Linear(64, 64)\n            self.qnet4 = nn.Linear(64, 7)\n\n        def forward(self, x):\n            x = F.relu(self.cnn0(x))\n            x = F.relu(self.cnn2(x))\n            x = nn.Flatten()(x)\n            x = F.relu(self.linear0(x))\n            x = F.relu(self.qnet0(x))\n            x = F.relu(self.qnet2(x))\n            x = self.qnet4(x)\n            x = x.argmax()\n            return x\n        \n        \n        #PPO\n        # will be slightly different","metadata":{"execution":{"iopub.status.busy":"2024-01-04T09:48:23.406989Z","iopub.execute_input":"2024-01-04T09:48:23.407385Z","iopub.status.idle":"2024-01-04T09:48:23.414786Z","shell.execute_reply.started":"2024-01-04T09:48:23.407351Z","shell.execute_reply":"2024-01-04T09:48:23.413787Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Overwriting submission.py\n","output_type":"stream"}]},{"cell_type":"code","source":"DQN_MLP.policy.state_dict().keys()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T09:30:04.527591Z","iopub.execute_input":"2024-01-04T09:30:04.527937Z","iopub.status.idle":"2024-01-04T09:30:04.534110Z","shell.execute_reply.started":"2024-01-04T09:30:04.527908Z","shell.execute_reply":"2024-01-04T09:30:04.533318Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"odict_keys(['q_net.features_extractor.cnn.0.weight', 'q_net.features_extractor.cnn.0.bias', 'q_net.features_extractor.cnn.2.weight', 'q_net.features_extractor.cnn.2.bias', 'q_net.features_extractor.linear.0.weight', 'q_net.features_extractor.linear.0.bias', 'q_net.q_net.0.weight', 'q_net.q_net.0.bias', 'q_net.q_net.2.weight', 'q_net.q_net.2.bias', 'q_net.q_net.4.weight', 'q_net.q_net.4.bias', 'q_net_target.features_extractor.cnn.0.weight', 'q_net_target.features_extractor.cnn.0.bias', 'q_net_target.features_extractor.cnn.2.weight', 'q_net_target.features_extractor.cnn.2.bias', 'q_net_target.features_extractor.linear.0.weight', 'q_net_target.features_extractor.linear.0.bias', 'q_net_target.q_net.0.weight', 'q_net_target.q_net.0.bias', 'q_net_target.q_net.2.weight', 'q_net_target.q_net.2.bias', 'q_net_target.q_net.4.weight', 'q_net_target.q_net.4.bias'])"},"metadata":{}}]},{"cell_type":"code","source":"th.set_printoptions(profile=\"full\")\n\nagent_path = 'submission.py'\n\nstate_dict = DQN_MLP.policy.to('cpu').state_dict()\nstate_dict = {\n    'cnn0.weight': state_dict['q_net.features_extractor.cnn.0.weight'],\n    'cnn0.bias': state_dict['q_net.features_extractor.cnn.0.bias'],\n    'cnn2.weight': state_dict['q_net.features_extractor.cnn.2.weight'],\n    'cnn2.bias': state_dict['q_net.features_extractor.cnn.2.bias'],\n    \n    'linear0.weight': state_dict['q_net.features_extractor.linear.0.weight'],\n    'linear0.bias': state_dict['q_net.features_extractor.linear.0.bias'],\n\n    'qnet0.weight': state_dict['q_net.q_net.0.weight'],\n    'qnet0.bias': state_dict['q_net.q_net.0.bias'],\n    'qnet2.weight': state_dict['q_net.q_net.2.weight'],\n    'qnet2.bias': state_dict['q_net.q_net.2.bias'],\n    'qnet4.weight': state_dict['q_net.q_net.4.weight'],\n    'qnet4.bias': state_dict['q_net.q_net.4.bias'],\n    \n    # PPO will be slightly different according to found policies\n}\n\nwith open(agent_path, mode='a') as file:\n    file.write(f'    state_dict = {state_dict}\\n')\n    \nprint('saved state_dict')","metadata":{"execution":{"iopub.status.busy":"2024-01-04T09:49:50.133979Z","iopub.execute_input":"2024-01-04T09:49:50.134340Z","iopub.status.idle":"2024-01-04T09:49:50.656837Z","shell.execute_reply.started":"2024-01-04T09:49:50.134309Z","shell.execute_reply":"2024-01-04T09:49:50.655770Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"saved state_dict\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile -a submission.py\n\n        \n    # How deep to make the game tree: higher values take longer to run!\n    N_STEPS = 3\n    \n    # Helper function for score_move: gets board at next step if agent drops piece in selected column\n    def drop_piece(grid, col, mark, config):\n        next_grid = grid.copy()\n        for row in range(config.rows-1, -1, -1):\n            if next_grid[row][col] == 0:\n                break\n        next_grid[row][col] = mark\n        return next_grid\n    \n    # Helper function for minimax: calculates value of heuristic for grid\n    def get_heuristic(grid, mark, config):\n        A = 1e6\n        B = 1e2\n        C = 1\n        D = -1e3\n        E = -1e11\n        num_twos = count_windows(grid, 2, mark, config)\n        num_threes = count_windows(grid, 3, mark, config)\n        num_fours = count_windows(grid, 4, mark, config)\n        num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n        num_fours_opp = count_windows(grid, 4, mark%2+1, config)\n        score = D*num_threes_opp + E*num_fours_opp + A*num_fours + B*num_threes + C*num_twos\n        return score\n\n    # Helper function for get_heuristic: checks if window satisfies heuristic conditions\n    def check_window(window, num_discs, piece, config):\n        return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n\n    # Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\n    def count_windows(grid, num_discs, piece, config):\n        num_windows = 0\n        # horizontal\n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[row, col:col+config.inarow])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(grid[row:row+config.inarow, col])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if check_window(window, num_discs, piece, config):\n                    num_windows += 1\n        return num_windows\n\n    # Uses minimax to calculate value of dropping piece in selected column\n    def score_move(grid, col, mark, config, nsteps):\n        next_grid = drop_piece(grid, col, mark, config)\n        score = minimax(next_grid, nsteps-1, False, mark, config,-np.inf, np.inf)\n        return score\n\n    # Helper function for minimax: checks if agent or opponent has four in a row in the window\n    def is_terminal_window(window, config):\n        return window.count(1) == config.inarow or window.count(2) == config.inarow\n\n    # Helper function for minimax: checks if game has ended\n    def is_terminal_node(grid, config):\n        # Check for draw \n        if list(grid[0, :]).count(0) == 0:\n            return True\n        # Check for win: horizontal, vertical, or diagonal\n        # horizontal \n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[row, col:col+config.inarow])\n                if is_terminal_window(window, config):\n                    return True\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(grid[row:row+config.inarow, col])\n                if is_terminal_window(window, config):\n                    return True\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if is_terminal_window(window, config):\n                    return True\n        # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if is_terminal_window(window, config):\n                    return True\n        return False\n\n    # Minimax implementation\n    def minimax(node, depth, maximizingPlayer, mark, config, alpha, beta):\n        is_terminal = is_terminal_node(node, config)\n        valid_moves = [c for c in range(config.columns) if node[0][c] == 0]\n        if depth == 0 or is_terminal:\n            return get_heuristic(node, mark, config)\n        if maximizingPlayer:\n            value = -np.Inf\n            for col in valid_moves:\n                child = drop_piece(node, col, mark, config)\n                value = max(value, minimax(child, depth-1, False, mark, config, alpha, beta))\n                alpha = max(alpha, value)\n                if beta <= alpha:\n                    break\n            return value\n        else:\n            value = np.Inf\n            for col in valid_moves:\n                child = drop_piece(node, col, mark%2+1, config)\n                value = min(value, minimax(child, depth-1, True, mark, config, alpha, beta))\n                beta = min(beta, value)\n                if beta <= alpha:\n                    break\n            return value\n\n    def get_minimax_move(obs, config):\n        # Get list of valid moves\n        valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n        # Convert the board to a 2D grid\n        grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n        # Use the heuristic to assign a score to each possible board in the next step\n        scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config, N_STEPS) for col in valid_moves]))\n        # Get a list of columns (moves) that maximize the heuristic\n        max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n        # Select at random from the maximizing columns\n        return random.choice(max_cols)\n\n    def check_winning_move_grid(grid, config, col, piece):\n        next_grid = drop_piece(grid, col, piece, config)\n        # horizontal\n        for row in range(config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(next_grid[row,col:col+config.inarow])\n                if window.count(piece) == config.inarow:\n                    return True\n        # vertical\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns):\n                window = list(next_grid[row:row+config.inarow,col])\n                if window.count(piece) == config.inarow:\n                    return True\n        # positive diagonal\n        for row in range(config.rows-(config.inarow-1)):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(next_grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n                if window.count(piece) == config.inarow:\n                    return True\n        # negative diagonal\n        for row in range(config.inarow-1, config.rows):\n            for col in range(config.columns-(config.inarow-1)):\n                window = list(next_grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n                if window.count(piece) == config.inarow:\n                    return True\n        return False\n\n    # Returns True if dropping piece in column results in game win\n    def check_winning_move(obs, config, col, piece):\n        # Convert the board to a 2D grid\n        grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n        return check_winning_move_grid(grid, config, col, piece)\n\n\n    def check_action(obs, config, action):\n        \n        valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n\n        # There is a wining move, don't wait for the model\n        for col in valid_moves:\n            if check_winning_move(obs, config, col, obs.mark):\n                return col\n\n        # if got no winning moves, check if your opponent has any winning moves. \n        for col in valid_moves:\n            if check_winning_move(obs, config, col, obs.mark%2+1):\n                return col\n            \n        grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n        if action in valid_moves:\n            # Try out the move and make sure it doesn't hand the enemy a win    \n            keep_checking = True\n            while keep_checking:\n                new_grid = drop_piece(grid, action, obs.mark, config)\n                if check_winning_move_grid(new_grid, config, action, obs.mark%2+1):\n                    valid_moves.remove(action)\n                    if len(valid_moves) > 0:\n                        action = get_minimax_move(obs, config)\n                    else:\n                        keep_checking = False\n                else:\n                    keep_checking = False\n\n            return int(action)\n        else:            \n            return get_minimax_move(obs, config)\n\n    model = Net()\n    model = model.float()\n    model.load_state_dict(state_dict)\n    model = model.to('cpu')\n    model = model.eval()\n\n    obs_tensor = tensor(obs['board']).reshape(1, 1, config.rows, config.columns).float()\n    obs_tensor = obs_tensor #/ 2 # Is the right?\n    action = model(obs_tensor)\n\n    action = check_action(obs, config, action)\n    \n    return int(action)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T09:49:54.283591Z","iopub.execute_input":"2024-01-04T09:49:54.283973Z","iopub.status.idle":"2024-01-04T09:49:54.296365Z","shell.execute_reply.started":"2024-01-04T09:49:54.283947Z","shell.execute_reply":"2024-01-04T09:49:54.295066Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Appending to submission.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Validate Submission & Battle Against Other Best","metadata":{}},{"cell_type":"code","source":"# load submission.py\nf = open(agent_path)\nsource = f.read()\nexec(source)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T09:50:13.103001Z","iopub.execute_input":"2024-01-04T09:50:13.103332Z","iopub.status.idle":"2024-01-04T09:50:13.355004Z","shell.execute_reply.started":"2024-01-04T09:50:13.103309Z","shell.execute_reply":"2024-01-04T09:50:13.353980Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Evaluate against negamax\nevaluate_agent(agent1=agent, agent2=\"negamax\", n_rounds=100)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T09:50:27.727815Z","iopub.execute_input":"2024-01-04T09:50:27.728163Z","iopub.status.idle":"2024-01-04T09:54:23.379057Z","shell.execute_reply.started":"2024-01-04T09:50:27.728132Z","shell.execute_reply":"2024-01-04T09:54:23.377230Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Agent 1 Win Percentage: 0.58\nAgent 2 Win Percentage: 0.36\nNumber of Invalid Plays by Agent 1: 0\nNumber of Invalid Plays by Agent 2: 0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate against previous best minimax\nevaluate_agent(agent1=agent, agent2=my_minimax_agent, n_rounds=100)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T09:54:23.380986Z","iopub.execute_input":"2024-01-04T09:54:23.381298Z","iopub.status.idle":"2024-01-04T09:59:37.716869Z","shell.execute_reply.started":"2024-01-04T09:54:23.381273Z","shell.execute_reply":"2024-01-04T09:59:37.715894Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Agent 1 Win Percentage: 0.32\nAgent 2 Win Percentage: 0.68\nNumber of Invalid Plays by Agent 1: 0\nNumber of Invalid Plays by Agent 2: 0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate against random\nevaluate_agent(agent1=agent, agent2=\"random\", n_rounds=100)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T09:59:37.718210Z","iopub.execute_input":"2024-01-04T09:59:37.718497Z","iopub.status.idle":"2024-01-04T10:00:19.366269Z","shell.execute_reply.started":"2024-01-04T09:59:37.718472Z","shell.execute_reply":"2024-01-04T10:00:19.364585Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Agent 1 Win Percentage: 1.0\nAgent 2 Win Percentage: 0.0\nNumber of Invalid Plays by Agent 1: 0\nNumber of Invalid Plays by Agent 2: 0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Congratulations!\n\nYou have completed the course, and it's time to put your new skills to work!  \n\nThe next step is to apply what you've learned to a **[more complex game: Halite](https://www.kaggle.com/c/halite)**.  For a step-by-step tutorial in how to make your first submission to this competition, **[check out the bonus lesson](https://www.kaggle.com/alexisbcook/getting-started-with-halite)**!\n\nYou can find more games as they're released on the **[Kaggle Simulations page](https://www.kaggle.com/simulations)**.\n\nAs we did in the course, we recommend that you start simple, with an agent that follows your precise instructions.  This will allow you to learn more about the mechanics of the game and to build intuition for what makes a good agent.  Then, gradually increase the complexity of your agents to climb the leaderboard!","metadata":{}}]}